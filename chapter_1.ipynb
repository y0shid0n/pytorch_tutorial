{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第1章 pytorchのきほん"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Tensor\n",
    "* Tensorはnumpyのndarrayとほぼ同様のAPI + GPUを利用できる\n",
    "* torch.Tensorは、torch.FloatTensorのエイリアスっぽい？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seedの固定\n",
    "# torchのシード固定は、実行直前に毎回セットしないとだめっぽい（randomとかでもそうだっけ...？）\n",
    "seed_value = 72\n",
    "# np.random.seed(seed_value)\n",
    "# random.seed(seed_value)\n",
    "# torch.manual_seed(seed_value)\n",
    "# torch.cuda.manual_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorの生成\n",
    "t = torch.Tensor([[1,2], [3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUにTensorを作成する\n",
    "# WSLだとGPUが認識できないっぽい\n",
    "t2 = torch.cuda.FloatTensor([[1,2], [3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ndarrayを渡し、倍精度のTensorを作る\n",
    "x = np.array([[1,2], [3,4]])\n",
    "t3 = torch.DoubleTensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0-9の数値で初期化された1次元のTensor\n",
    "#t4 = torch.arange(0, 10) # どちらでもいい\n",
    "t4 = torch.Tensor(np.arange(0,10))\n",
    "t4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# すべての値が0の100x10のTensorを作成し、cudaメソッドでGPUに転送する\n",
    "t5 = torch.zeros(100,10).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正規乱数で100x10のTensorを生成\n",
    "t6 = torch.randn(100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 10])\n",
      "torch.Size([100, 10])\n"
     ]
    }
   ],
   "source": [
    "# Tensorのshapeはsizeメソッドで取得可能\n",
    "# shapeでも可\n",
    "print(t6.size())\n",
    "print(t6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [3., 4.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpyメソッドを使用してndarrayに変換\n",
    "t7 = torch.Tensor([[1, 2], [3, 4]])\n",
    "x7 = t.numpy()\n",
    "x7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [3., 4.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU上のTensorはcpuメソッドで一度CPUのtensorに変換しないとndarrayにはできない\n",
    "t8 = t7.cuda()\n",
    "x8 = t8.cpu().numpy()\n",
    "x8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "tensor(3.)\n",
      "tensor([[1., 2.],\n",
      "        [4., 5.]])\n",
      "tensor([[2., 3.],\n",
      "        [5., 6.]])\n",
      "tensor([4., 5., 6.])\n",
      "tensor([[  1., 100.,   3.],\n",
      "        [  4.,   5.,   6.]])\n",
      "tensor([[  1., 200.,   3.],\n",
      "        [  4., 200.,   6.]])\n",
      "tensor([[ 1., 20.,  3.],\n",
      "        [ 4., 20.,  6.]])\n"
     ]
    }
   ],
   "source": [
    "# Tensorは基本的にndarrayと同様のインデックス操作が可能\n",
    "t9 = torch.Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(t9)\n",
    "print(t9[0, 2])\n",
    "print(t9[:, :2])\n",
    "print(t9[:, [1,2]])\n",
    "print(t9[t9 > 3])\n",
    "t9[0, 1] = 100\n",
    "print(t9)\n",
    "t9[:, 1] = 200\n",
    "print(t9)\n",
    "t9[t9 > 10] = 20\n",
    "print(t9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11., 12., 13.])\n",
      "tensor([1., 4., 9.])\n",
      "tensor([  1.,  -8., -17.])\n",
      "tensor([ 8.,  9., 10.])\n",
      "tensor([[  0.,   2.,   4.],\n",
      "        [ 20., 400., 600.]])\n",
      "tensor([[  1.,   3.,   5.],\n",
      "        [ 11., 202., 303.]])\n",
      "tensor([[  0.,   2.,   4.],\n",
      "        [ 20., 400., 600.]])\n"
     ]
    }
   ],
   "source": [
    "# 四則演算はTensor同士、またはTensorとスカラーでのみ可能\n",
    "# Tensorとndarrayは混在できない\n",
    "# Tensor同士でも型は一致させる必要がある\n",
    "# ndarray同様に、Tensornにもブロードキャスト（次元の補完）が適用される\n",
    "\n",
    "v = torch.Tensor([1, 2, 3])\n",
    "w = torch.Tensor([0, 10, 20])\n",
    "m = torch.Tensor([[0, 1, 2], [10, 200, 300]])\n",
    "\n",
    "v2 = v + 10\n",
    "print(v2)\n",
    "v2 = v ** 2\n",
    "print(v2)\n",
    "\n",
    "z = v - w\n",
    "print(z)\n",
    "\n",
    "u = 2 * v - w / 10 + 6.0\n",
    "print(u)\n",
    "\n",
    "m2 = m * 2.0\n",
    "print(m2)\n",
    "\n",
    "m3 = m + v\n",
    "print(m3)\n",
    "\n",
    "m4 = m + m\n",
    "print(m4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0206)\n",
      "tensor(0.0206)\n",
      "tensor([-0.0733,  0.1366,  0.1345,  0.0627,  0.0248, -0.0725,  0.0262,  0.1065,\n",
      "        -0.0265, -0.1127])\n"
     ]
    }
   ],
   "source": [
    "# 各種関数も使える\n",
    "torch.manual_seed(seed_value) # seed固定\n",
    "X = torch.randn(100, 10)\n",
    "\n",
    "# 絶対値\n",
    "y = X * 2 + torch.abs(X)\n",
    "# 平均\n",
    "m = torch.mean(X)\n",
    "print(m)\n",
    "# 関数ではなくメソッドとしても使える\n",
    "m = m.mean()\n",
    "print(m)\n",
    "# 集計は次元を指定できる\n",
    "m2 = X.mean(axis = 0)\n",
    "print(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[10., 20., 30.],\n",
      "        [40., 50., 60.]])\n",
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[10., 40.],\n",
      "        [20., 50.],\n",
      "        [30., 60.]])\n",
      "tensor([[ 1.,  2., 10., 20., 30.],\n",
      "        [ 3.,  4., 40., 50., 60.]])\n",
      "torch.Size([100, 64, 32, 3])\n",
      "torch.Size([100, 3, 64, 32])\n"
     ]
    }
   ],
   "source": [
    "# 次元の操作\n",
    "x1 = torch.Tensor([[1, 2], [3, 4]]) # 2x2\n",
    "x2 = torch.Tensor([[10, 20, 30], [40, 50, 60]])\n",
    "print(x1)\n",
    "print(x2)\n",
    "\n",
    "# 2x2を4x1に見せる\n",
    "print(x1.view(4, 1))\n",
    "# -1は残りの次元を表し、一度だけ使用できる\n",
    "# 以下の場合、最初に1を指定したので残りは4\n",
    "print(x1.view(1, -1))\n",
    "\n",
    "# 転置\n",
    "print(x2.t())\n",
    "\n",
    "# dim=1に対して結合することで、2x5のTensorを作る\n",
    "print(torch.cat([x1, x2], dim=1))\n",
    "\n",
    "# HWCをCWHに変換\n",
    "# 64x32x3のデータが100個\n",
    "torch.manual_seed(seed_value)\n",
    "hwc_img_data = torch.rand(100, 64, 32, 3)\n",
    "print(hwc_img_data.shape)\n",
    "chw_img_data = hwc_img_data.transpose(1, 2).transpose(1, 3)\n",
    "print(chw_img_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 10])\n",
      "torch.Size([10])\n",
      "tensor(16.4310)\n",
      "torch.Size([100])\n",
      "torch.Size([10, 10])\n",
      "torch.Size([100, 10])\n",
      "tensor([13.0095, 12.0607, 11.4284, 11.1951, 10.2369,  9.3851,  9.0325,  8.5119,\n",
      "         8.2606,  7.3641])\n",
      "torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "# 行列の演算\n",
    "torch.manual_seed(seed_value)\n",
    "m = torch.randn(100, 10)\n",
    "torch.manual_seed(seed_value)\n",
    "v = torch.randn(10)\n",
    "print(m.shape)\n",
    "print(v.shape)\n",
    "\n",
    "# 内積\n",
    "d = torch.dot(v, v)\n",
    "print(d)\n",
    "\n",
    "# 行列とベクトルの積\n",
    "v2 = torch.mv(m, v)\n",
    "print(v2.shape)\n",
    "\n",
    "# 行列積\n",
    "m2 = torch.mm(m.t(), m)\n",
    "print(m2.shape)\n",
    "\n",
    "# 特異値分解\n",
    "u, s, v = torch.svd(m)\n",
    "print(u.shape)\n",
    "print(s)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Variableと自動微分\n",
    "\n",
    "**どうやらVariableはpytorch0.4からTensorに統合されたらしく、Tensorをそのままbackwardできるようになったもよう**\n",
    "\n",
    "https://qiita.com/vintersnow/items/91545c27e2003f62ebc4\n",
    "\n",
    "**Variableはdeprecatedなので、使わないほうがよい**\n",
    "\n",
    "https://pytorch.org/docs/stable/autograd.html#variable-deprecated\n",
    "\n",
    "* variableはTensorを拡張し、自動微分を扱えるようにしたもの\n",
    "* NNではパラメータやデータは全てVariableを使用する\n",
    "* Tensorの演算はそのまま利用できるが、TensorとVariableを混ぜて演算することはできない\n",
    "* VariableのdataプロパティにアクセスするとTensorを取り出せる\n",
    "* Variableに対して演算を積み重ねると計算グラフが構築され、backwardメソッドを呼ぶと自動的に微分を計算できる\n",
    "* NNの最適化なんかで重要らしい（わからん）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable as V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6095,  0.0248, -0.6509],\n",
      "        [-1.2133, -0.9720,  0.5561],\n",
      "        [-0.8389, -2.5504, -0.6438]])\n",
      "tensor([[-0.6095,  0.0248, -0.6509],\n",
      "        [-1.2133, -0.9720,  0.5561],\n",
      "        [-0.8389, -2.5504, -0.6438]])\n",
      "torch.float32\n",
      "tensor([1., 2., 3.], requires_grad=True)\n",
      "tensor([1., 2., 3.], requires_grad=True)\n",
      "torch.float32\n",
      "torch.Size([100])\n",
      "tensor([-2.5125, -1.4888, -7.8712,  0.7108,  3.9587, -4.7207, -5.9617, -3.2727,\n",
      "        -1.1687,  1.6212,  0.8945,  5.2887, -1.2928, -4.7624, -5.4628,  3.4641,\n",
      "        -5.0585, -0.9466, -0.8145, -1.9913, -8.6125, -0.5781, -5.9557,  2.3592,\n",
      "         7.0098,  1.0560,  1.0235, -6.0381, -8.7797,  3.3768,  0.4492, -7.9153,\n",
      "         7.5558,  6.8641, -2.9943,  1.3821, -4.1092,  1.5527,  0.5417,  0.4636,\n",
      "         0.8153, -3.9358,  0.3027, -2.1342, -1.7281,  4.5834, -1.4723, -3.1393,\n",
      "         3.5540,  0.9848,  4.4618, -5.5899,  1.4705,  1.7438,  1.8758, -1.2955,\n",
      "        -5.4876, 10.5755, -5.9045, -6.7668, -0.5909, -2.8809,  0.7169,  7.2164,\n",
      "         4.9053, -4.6263, -0.6914, -0.3212,  1.6678,  5.2473,  1.9645, -1.2054,\n",
      "        -0.8732,  1.6530,  4.4037,  3.7830, -4.8337,  1.1593, -3.9302,  1.7013,\n",
      "        -2.3676, -0.8134,  1.0030, -4.2350, 10.0008,  1.2978,  3.5910, -2.9614,\n",
      "         0.1725, -0.4747, -0.8156,  0.1283, -0.3961, -4.4276, -3.0406, -3.0513,\n",
      "        -2.6094,  1.4665,  0.5116,  4.1617], grad_fn=<MvBackward>)\n",
      "tensor(-38.2163, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed_value)\n",
    "_x = V(torch.randn(100, 3))\n",
    "print(_x[:3])\n",
    "# Variableは使わなくてよい\n",
    "torch.manual_seed(seed_value)\n",
    "x = torch.randn(100, 3)\n",
    "print(x[:3])\n",
    "print(x.dtype)\n",
    "\n",
    "# 微分の変数として扱う場合はrequires_gradフラグをTrueにする\n",
    "_a = V(torch.Tensor([1, 2, 3]), requires_grad=True)\n",
    "print(_a)\n",
    "# Variableを使わない書き方\n",
    "# torch.Tensorではなくtorch.tensorを使い、dtypeとrequires_gradを指定する\n",
    "# 渡すリストの中身をfloatにしておけばdtypeを指定しなくてもよい\n",
    "# a = torch.tensor([1, 2, 3], dtype=torch.float32, requires_grad=True)\n",
    "a = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "print(a)\n",
    "print(a.dtype)\n",
    "\n",
    "# 計算をすることで自動的に計算グラフが構築される\n",
    "# ここでy = xia1 + yia2 + zia3がi=1～i=100まで作られる\n",
    "y = torch.mv(x, a)\n",
    "print(y.shape)\n",
    "print(y)\n",
    "\n",
    "# ここでy = a1(x1 + x2 + x3 + ... + x100) + a2(y1 + y2 + y3 + ... + y100) + a1(z1 + z2 + z3 + ... + z100)\n",
    "o = y.sum()\n",
    "print(o)\n",
    "\n",
    "# 微分を実行\n",
    "o.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-11.4051,   4.9093, -12.2099])\n",
      "tensor([-11.4051,   4.9093, -12.2099])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 解析解と比較\n",
    "print(a.grad) # oをaで微分するので、xi, yi, ziのsumが残る\n",
    "print(x.sum(0))\n",
    "print(x.grad is None) # xはrequires_gradがFalseなので微分は計算されない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5., grad_fn=<AddBackward0>)\n",
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# もうちょい単純な例（下記より）\n",
    "# https://qiita.com/mananam/items/f4be3fb0d996a6a3eae3\n",
    "# テンソルを作成\n",
    "# requires_grad=Trueで自動微分対象を指定\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# 計算グラフを構築\n",
    "# y = 2 * x + 3\n",
    "y = w * x + b\n",
    "print(y)\n",
    "\n",
    "# 勾配を計算\n",
    "y.backward()\n",
    "\n",
    "# 勾配を表示\n",
    "print(x.grad)  # dy/dx = w = 2\n",
    "print(w.grad)  # dy/dw = x = 1\n",
    "print(b.grad)  # dy/db = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
